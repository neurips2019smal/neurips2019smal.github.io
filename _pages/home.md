---
layout: project
urltitle:  "Scientific Models and Machine Learning (SMMaL)"
title: "Scientific Models and Machine Learning (SMMaL)"
categories: nips, neurips, vancouver, canada, workshop, mathematics, machine learning, smal, 2019, neurips19
permalink: /
<!-- favicon: /static/img/embodiedqa/favicon.png -->
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Scientific Models and Machine Learning (SMMaL)</h1></center>
    <center><h2>NeurIPS 2019 Workshop, Vancouver, Canada</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;">
      Friday, 13th December, 08:30 AM to 06:30 PM,
      Room: to be specified
    </span></center>
  </div>
</div>

<hr>

<!-- <div class="row" id="intro"> -->
  <!-- <div class="col-md-12"> -->
    <!-- <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}"> -->
    <!-- <p> Image credit: [2, 28, 12, 11, 15-21, 26]</p> -->
  <!-- </div> -->
<!-- </div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
<p>
Machine Learning with Deep Neural Networks has been progressively adapted as a computational tool in the natural sciences and mathematics. Significant advances have been made in enhancing numerical simulation methods with ML and solving complex dynamical equations. Deep models are also accelerating progress in other mathematical sciences, such as electrical and computer engineering, medicine, chemistry, and biology.  However, a complete understanding of the dynamics of DNNs themselves is lacking, leading researchers to question their fundamental understanding of this progress and its sustainability.  This workshop aims to address a tension between two viewpoints, one in which machine learning is used as a tool for progress and understanding and another that aims to understand why the tool is so effective.  We aim to provide a venue to explore the give-and-take relationship between the mathematical sciences and ML with the aim of gaining deeper insight into the mathematics of ML and how/when to use ML algorithms to solve mathematical problems in science.</p>

<!--This workshop is placed in the tension between the two viewpoints, exploring the take-and-give relationship between mathematical sciences and ML with the aim of gaining deeper insight into the mathematics of ML, and using ML algorithms to solve mathematical problems in science.-->

<p>
We aim to bring together researchers across the mathematical sciences to discuss and learn about the exchange in both directions: 1) how domain-specific mathematical models are being used to understand machine learning and 2) how machine learning is being used to solve problems in domain-specific models.  The primary objective is to understand where our fundamental knowledge in these domains and machine learning overlap and where they deviate and to understand the nature of this deviation.</p>

<p>Examples of 1) include the use of Fokker-Plank equation and Stochastic Differential Equations to understand Stochastic Gradient Descent <a href="#note1" id="note1ref">[1-2]</a>, the use of Game Theory to provide robustness to trained networks to adversarial attacks <a href="#note3" id="note3ref">[3]</a> or Complexity Theory to understand how Recurrent Networks can achieve super Universal Turing Machine capabilities <a href="#note4" id="note4ref">[4-5]</a>. In each case, a well established mathematical foundation exists that can help enquire about the performance of machine learning models. Examples of 2) include using machine learning to solve the the Quantum Many-Body Problem  <a href="#note6" id="note6ref">[6]</a>,  or using deep networks to find solutions of high dimensional Partial Diferential Equations <a href="#note7" id="note7ref">[7]</a>. Therefore tackling long standing mathematical questions taking advantage of the features introduced by machine learning.
</p>

</div>
</div> <br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>November 1st, 2018 - Midnight Pacific Time</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>November 9th, 2018 </td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>December 7th, 2018</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We invite high-quality paper submissions on the following topics:
    </p>

  <p>
  <ul>
  <li>novel ways to use AI to understand mathematical models</li>
  <li>novel ways to use scientific models to understand AI</li>
  <li>stochastic differential equations to model gradient flow</li>
  <li>deep learning to solve long standing physics problems</li>
  <li>deep learning for high-dimensional Partial Differential Equations</li>
</ul>
    </p>


  <p> <span style="font-weight:500;">Submission:</span>
  <br/>
    Accepted papers
    will be presented during joint poster sessions, with exceptional submissions
    selected for spotlight oral presentation. Accepted papers will be made publicly
    available as non-archival reports,
    allowing future submissions to archival conferences or journals.
  </p>

  <p>
  Submissions should be up to 4 pages excluding references, acknowledgements, and supplementary material, and should be
      <span style="color:#1a1aff;font-weight:400;"><a href="https://nips.cc/Conferences/2018/PaperInformation/StyleFiles">NIPS format</a></span> and anonymous. The review process is double-blind.
  </p>
  <p>
  We also welcome published papers that are within the scope of the workshop (without re-formatting). This specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.
  </p>

  <p>
  Please submit your paper to the following address: <a href="https://cmt3.research.microsoft.com/VIGIL2018">https://cmt3.research.microsoft.com/VIGIL2018</a>
  If you have any question, send an email to: vigilworkshop2018@gmail.com
  </p>

  <p><b>Accepted workshop papers are eligible to the pool of reserved conference tickets (one ticket per accepted papers).</b>
  </p>

  </div>

</div><br>




<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>08:30 AM : Opening Remarks                        </li>
      <br>
      <p><b>Scientific Models to understand Deep Learning</b></p>
      <li>08:40 AM : Invited Speaker 1: Surya Ganguli - An analytic theory of generalization dynamics and transfer learning in deep linear networks  | <a href="https://google.com">Slides (still unavailable)</a> </li>
      <li>09:20 AM : Invited Speaker 2: Terrence Tao - On the universality of the incompressible Euler equation on compact manifolds, II. Non-rigidity of Euler flows   | <a href="https://google.com">Slides (still unavailable)</a> </li>
      <li>10:00 AM :  </li>
      <li>10:15 AM : Spotlights (6*2min)
      <ul>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <!-- <li></li> -->
      </ul>
      </li>
      <li>10:30 AM : Coffee Break (20min)                   </li>
      <li>10:50 AM : Invited Speaker 3: Aasa Feragen - Geometry and Statistics: Manifolds and Stratified Spaces  | <a href="https://google.com">Slides (still unavailable)</a>  </li>
      <li>11:30 AM : Invited Speaker 4: Hava Siegelmann - Analog computation via neural networks | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>12:10 PM : Poster Session <b>Lunch provided!</b> </li>
      <li>01:10 PM : Break                                  </li>
      <br>
      <p><b>Deep Learning to understand Scientific Models</b></p>
      <li>01:40 PM : Invited Speaker 5: Konstantinos Spiliopoulos - DGM: A deep learning algorithm for solving partial differential equations  | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>02:20 PM : Invited Speaker 6: Matthias Troyer - DGM: A deep learning algorithm for solving partial differential equations.      | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>03:00 PM : Coffee Break & Poster Session (50 mins)</li>
      <li>03:50 PM : Invited Speaker 7: Weinan E - DeePMD-kit: A deep learning package for many-body potential energy representation and molecular dynamics   | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>04:30 PM : Invited Speaker 8: Amy Greenwald - Learning Equilibria of Simulation-Based Games | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>05:10 PM : Panel Discussion                       </li>
      <li>06:00 PM : Closing Remarks                        </li>
    </ul>
  </div>
</div>


<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/terencetao.png" | prepend:site.baseurl }}">
    <p><b>Terence Tao</b>
    is Professor of mathematics at the University of California, Los Angeles, and was awarded the 2006 Fields Medal.
    His research focuses on harmonic analysis, partial differential equations, algebraic combinatorics, arithmetic combinatorics, geometric combinatorics, compressed sensing and analytic number theory.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.math.ucla.edu/~tao/">Webpage</a>]</span></p>
  </div>
</div><br>



<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/aasaferagen.jpg" | prepend:site.baseurl }}">
    <p><b>Aasa Feragen</b>
    is Professor of Computer Science at the University of Copenhagen. She is interested in uncertainty quantification for functional data in medical imaging, statistics and machine learning for data with topological variation (trees, graphs, point sets, functions, images) and nonlinear statistics; statistics in metric spaces such as Riemannian manifolds, stratified spaces, spaces of trees and graphs. She works as well on applications in medical imaging and computer visison.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://sites.google.com/site/aasaferagen/">Webpage</a>]</span></p>
  </div>
</div><br>




<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/siegelmann.jpg" | prepend:site.baseurl }}">
    <p><b>Hava T. Siegelmann</b>
    is Professor of Computer Science at University of Massachusetts Amherst. She's a recognized expert in Complex Systems and Neural Networks, focuses on theoretical computational neuroscience, computation in and modeling of natural systems and their application to intelligent systems. Of particular research interest are intelligence vis-a-vis adaptive memory, advanced models of cognition, and evolving, intelligent interfaces for robotics and other intelligent systems. Her studies often involve multi-scale modeling and system level analysis of major disorders such as cancer. The creator of a new field of computer science, Super-Turing computation, Dr. Siegelmann is applying the theory to biological systems and exploring them in connection with a new generation of analog computer.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.cics.umass.edu/faculty/directory/siegelmann_hava">Webpage</a>]</span></p>
  </div>
</div><br>






<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/suryaganguli.jpg" | prepend:site.baseurl }}">
    <p><b>Surya Ganguli</b>
    is Professor of applied physics at Stanford University, Los Angeles, and was awarded the 2016 Investigator Award in Mathematical Modeling of Living Systems by Simons Foundation.
      His research is focused on understanding Machine Learning and Neuroscience with techniques from a wide range of disciplines such as statistical mechanics, dynamical systems theory and information theory.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://ganguli-gang.stanford.edu/surya.html">Webpage</a>]</span></p>
  </div>
</div><br>


<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/amygreenwald.png" | prepend:site.baseurl }}">
    <p><b>Amy R. Greenwald</b>
    is Professor of Computer Science at Brown University. Her work splits in twin research goals: first, the effort to design and implement AI agents that interact effectively in multiagent environments; second, the effort to understand, explain, and accurately predict the dynamics of such interactions. In pursuing these goals, Prof. Greenwald draws from theoretical and practical sources, including a variety of disciplines such as AI, decision theory, game theory, and economics.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://cs.brown.edu/people/faculty/amy/">Webpage</a>]</span></p>
  </div>
</div><br>


<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/konstantinosspiliopoulos.jpg" | prepend:site.baseurl }}">
    <p><b>Konstantinos Spiliopoulos</b>
    is Professor at the Department of Mathematics and Statistics, Boston University. His research lies broadly in the area of stochastic processes, applied mathematics and probability, large deviations, multiscale methods, financial mathematics, asymptotic problems for stochastic processes and partial differential equations , statistical inference for stochastic differential equations and statistical learning.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://math.bu.edu/people/kspiliop/">Webpage</a>]</span></p>
  </div>
</div><br>



<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/matthiastroyer.jpg" | prepend:site.baseurl }}">
    <p><b>Matthias Troyer</b>
    is been professor at ETH Zurich and has now joined Microsoft’s quantum computing program. He works on a variety of topics in quantum computing, from the simulation of materials and quantum devices to quantum software, algorithms and applications of future quantum computers. His broader research interests span from high performance computing and quantum computing to the simulations of quantum devices and island ecosystems.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.microsoft.com/en-us/research/people/mtroyer/">Webpage</a>]</span></p>
  </div>
</div><br>


<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/weinane.jpg" | prepend:site.baseurl }}">
    <p><b>Weinan E</b>
    is Professor in Applied and Computational Mathematics at Princeton University.
    He has worked on various disciplines of sciences and has contributed to the resolution of some long standing scientific problems such as the Burgers turbulence problem, the Cauchy-Born rule for crystalline solids, and the moving contact line problem. He is interested in bringing clarity to scientific issues through mathematics and in multi-scale and/or multi-physics problems. He has also worked on building the mathematical framework and finding effective numerical algorithms for modeling rare events and has developed diverse tools to analyze algorithms.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://web.math.princeton.edu/~weinan/">Webpage</a>]</span></p>
  </div>
</div><br>



<div class="row" id="recordings">
    <div class="col-xs-12">
    <h2>Recordings</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <p>
    The workshop was broadcasted via BlueJeans. You can find the recordings here: 
    <a href="https://bluejeans.com/s/KAsSh">Morning Session</a> | <a href="https://bluejeans.com/s/hEo5B">Afternoon Session</a></p>
  </div>
</div>

<div class="row" id="accepted-papers">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row">
<div class="col-xs-12">
  <ul>
    <li></li>
    <li></li>
    <li></li>

  </ul>
</div>
</div>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  
  <div class="col-xs-3">
    <a href="http://rlily.hu/">
      <img class="people-pic" src="{{ "/static/img/people/Lily.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://rlily.hu/">Lily Hu</a>
      <h6>Salesforce Research</h6>
      <h6>University of California, Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://thorjonsson.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/thor.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://thorjonsson.github.io/">Thor Jonsson</a>
      <h6>University of Guelph</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://lucehe.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/lucaherrtti.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://lucehe.github.io/">Luca Herrtti</a>
      <h6>Université de Sherbrooke</h6>
    </div>
  </div>
  
  <div class="col-xs-3">
    <a href="https://www.ece.rutgers.edu/jorge-ortiz">
      <img class="people-pic" src="{{ "/static/img/people/jortiz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ece.rutgers.edu/jorge-ortiz">Jorge Ortiz</a>
      <h6>Rutgers, The State University of New Jersey</h6>
    </div>
  </div>
  </div>

<hr>



<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-12 sponsor">
    <a href="https://merl.com/">
      <img src="{{ "/static/img/ico/merl-logo-big.jpg" | prepend:site.baseurl }}">
    </a>
    <a href="https://deepmind.com/">
      <img src="{{ "/static/img/ico/deepmind_logo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://ai.google/">
      <img src="{{ "/static/img/ico/googlelogo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://iglu-chistera.github.io/">
      <img src="{{ "/static/img/ico/logoIGLU-300.png" | prepend:site.baseurl }}">
    </a>
    <a href="http://uber.ai/">
      <img src="{{ "/static/img/ico/logo_uber.jpg" | prepend:site.baseurl }}">
    </a>
  </div>
</div>
<br>


<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> and
      <span style="color:#1a1aff;font-weight:400;"> <a href="nips2018vigil.github.io">nips2018vigil.github.io</a></span>
      for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <a id="note1" href="#note1ref">[1]</a> Q. Li, C. Tai and W. E 
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1511.06251.pdf">Stochastic modified equations and adaptive stochastic gradient algorithms.</a></span> 
  ICML, 2017.

  <br><a id="note1" href="#note1ref">[2]</a> P. Chaudhari and S. Soatto  
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1710.11029.pdf">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.</a></span> 
  ICLR, 2018.

  <br><a id="note3" href="#note3ref">[3]</a> G. S. Dhillon1, K. Azizzadenesheli, Z. C. Lipton, J. Bernstein, J. Kossaifi, A. Khanna, A. Anandkumar
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=H1uR4GZRZ">Stochastic Activation Pruning for Robust Adversarial Defense.</a></span> 
  ICLR, 2018.
  
  

  
  <br><a id="note4" href="#note4ref">[4]</a> H.T. Siegelmann and E.D. Sontag
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://pdf.sciencedirectassets.com/272574/1-s2.0-S0022000000X00360/1-s2.0-S0022000085710136/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEIP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIG7N1TvjUHLWQrdS8tQz3KTq2a1Tj4xGM58709NGZjTUAiEAg9EROQsxFp%2F%2FaSRR2CLGjGFF3af76eh64gnUnO%2B14Ukq4wMIrP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDGcHqSFyN5NbhLF2JCq3A49Xw3XDjCOBugnZhO0PdiVRwbLwWrOpy8ErrabGYltqbK7D0ayZCutvH4OHhW1iflrPgatXtJsp1iEnCPj%2BxvbS%2Fc2zsOGDEdxyEMiE47un2GXKovQOTrwmqox8fEuFK0Ip6cLby%2F4fPscc5FV9uTwEyUcbX6dMEcIdeJW1VvAKZPucFX6xHQYDDPfis8Rb3vw8%2F6RjpVI4kLrp%2FdyYzL9UgjwrAWMhelbkPqbd7qnCXHBQTAWdMox5DFuJy2bWfuLpm%2FYTJgRTb2JNvO%2BkgaHrdhoEpGCANq5XdVN8T3COo4%2B8SXAg1X%2FcyMf5JD0nsW1We6myTDnF%2FDoub4UQIN3l4loxC1dwz9dyWBWODknHdzlDPagu%2FiRJfgwIYwVcKgYNaa7a0NlcFPAOgTuKb1IHSClmlfnQLPOuIfgys9ckGcsZ%2FZnhESM4DaozCeqj%2FnPx0NtSotfquXVJoXkS9h7S1WG1nJ4ZZ%2BaNsQLbmV3dVZoyNqPNLkzSTBE8fNNLspqSxRd9ZQm6dXeJYbTGEFP3azqZ86BtSpxNaUEG1bYyp5hCO%2BUHHT5LxE05Iub5GgksL2WvcQQw7O3%2F5wU6tAFyMBLu0kulw3zYCyB9qOF0%2FuhAUXfead08i%2BZAcafQKS92boEElJN5cUop0NXOnsdnoZNrecaFhwUptgBrrtYPOwnLKs3ifhucSrTg3xG%2BHEOF%2BLnB4qGmslweCS2UHAwoPxUXgCMNWQaWTFAhNnpMxDbAdtGpI7tOc2pmZbF1kiR%2Bk%2BZFXqWyeoN649nKg41xJbBF9XbcsR16nKN%2BiQgV9%2F3VNcL4Up9Wp0NUCiBHMEsZ8Yg%3D&AWSAccessKeyId=ASIAQ3PHCVTY24J2UZLE&Expires=1560280614&Signature=2EA9ajQ7E7lxnKGcVfOUEIznF2Q%3D&hash=ea8753eb8f32e979b698ea2e36e2cf36a9f1b28926044de04b6a889359131625&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0022000085710136&tid=spdf-ae0b997b-814f-400f-8aba-4ae0a00dc595&sid=406effae72007249516a78029a955bfad9dfgxrqa&type=client">On the Computational Power of Neural Nets.</a></span> 
  Science, 1995.
  
  <br><a id="note5" href="#note5ref">[5]</a> H.T. Siegelmann and E.D. Sontag
  <span style="color:#1a1aff;font-weight:400;"> <a href="http://www.sontaglab.org/FTPDIR/siegelmann_sontag_tcs1994.pdf">Analog computation via neural networks.</a></span> 
  Science, 1994.  
  
  
  
  
  
  
  
  
  <br><a id="note6" href="#note6ref">[6]</a> G. Carleo and M. Troyer
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://science.sciencemag.org/content/355/6325/602">Solving the Quantum Many-Body Problem with Artificial Neural Networks.</a></span> 
  Science, 2017.
  
  <br><a id="note7" href="#note7ref">[7]</a> J. Sirignano and K. Spiliopoulos  
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://www.sciencedirect.com/science/article/pii/S0021999118305527">DGM: A deep learning algorithm for solving partial differential equations.</a></span>
  Science, 2018.
  
  <br><a id="note8" href="#note8ref">[8]</a> R. Porotti, D. Tamascelli, M. Restelli, E. Prati 
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/abs/1901.06603">Coherent Transport of Quantum States by Deep Reinforcement Learning.</a></span>
  ArXiv, 2019.

  <br>[9] T. Wu, M. Tegmark
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1810.10525.pdf">Toward an AI Physicist for Unsupervised Learning.</a></span>
  ArXiv, 2018.

  <br>[10] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=H1W1UN9gg">Deep Information Propagation.</a></span>
  ICLR, 2017.


  <br>[11] Z. Nado, J. Snoek, B. Xu, R. Grosse, D. Duvenaud, and J. Martens
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=ry-Se9kvG
">Stochastic Gradient Langevin Dynamics that Exploit Neural Network Structure.</a></span>
  ICLR, 2018.

  <br>[12] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar, N. Samatova, and V. Kumar
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1612.08544.pdf">Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data.</a></span>
  IEEE Transactions on Knowledge and Data Engineering, 2017.


</div>

<br><br><br><br><br><br><br>



